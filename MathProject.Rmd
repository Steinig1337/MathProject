---
title: "MathProject"
author: "F D"
date: "26 12 2019"
output:
  rmdformats::readthedown:
    highlight: kate
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Load the packages

```{r include=FALSE}
library(tidyverse)
library(readxl)
library(shiny)
library(shinythemes)
library(lubridate)
```

# Executive summary

## The Questions

**Are there dependencies betweeen the weather and  the sales?**

I couldn't answer this question because i was running out of time.

**Are there dependencies between the weather and different product sales?**

I couldn't answer this question because i was running out of time.

**Are there dependencies between the weather and customers?**

No, i could not find any dependencies between the customers and the weather because customers are in severall clusters. So they don't care about the weather. But i found a dependency between the average money spend by customers and the weather. If the weather gets cold and wet the customers buy more superfood.

## Data Wrangling

I had lots of different data from excel sheets. So first of all i load them into rstudio and combined the different months of the 3 different types. After that i had tree different data frames. Because i changed the names of the offers on the website and some other things. I got serveral names for one product. So i decided to use the package stringr to replace the different names with the actual name. I did the same for the product numbers. It called numbers but in there are strings too. I got one problem with the brokkoli name. In the test appears the name twice although it is the same name. I tested the names and product numbers, so my data was almost ready for an analysis. Before i startet to analys the data i changed the date column with the lubridate package to get an equal date column for a join. I joined the weather data to the three different data frames by the date column. Now i was ready for the statistical analysis.

## Statistical analysis

### Kmeans with ggplot unscaled and 2d plot

I started with the kmeans clustering like we did it in class with ggplot and a 2d plot. I clustered the data and added a column with the clusters to make a plot which shows us the clusters. The data wasn't scaled and the plot looked not really good. I tried this for all three data frames and got nothing good. The clusters weren't there and the points were on one line.

### Factoextra package with PCA

After that i was thinking:"There must be a better way to do that". So i decided to use the factoextra package witch we used in class too. Here i started with the principal component analysis because i wanted to find the components whitch cover most of the variance. So i could delete some of the attributes and maybe the clusters and the results are getting better. I used the PCA method with all numeric variables of the customer_weather data frame and looked on the eigenvalues and eigenvariances with a table and a plot. You could see that we cover around 80 % of the variance with only 4 dimensions and 86 % with 5 dimensions. After that i wanted to look deeper into it. I tried to find the variables witch hold most of the variance. For that i uses the variable plot with the dimension 1 and 2 and the corrolation plot with up to 5 dimensions. You saw that TXK, TNK, VPM, TMK, UPM and TGK explained most of the variance in dimensions 1 and there were mostly only in dimension 1. FX, FM, RSK, NM and PM were found in serveral dimensions and explained so the second, the third and the fifth dimension. I decided here to use only the dimension 1 and 4. One because of the above-mentioned properties and 4 because thats the only numeric variable for the sales. In the end i plotted the bipolarplot witch shows us the direction and contribution of dimension 1 and 2 and all the customers who bought something on one day. You saw lots of groups of points. Each point was one customer and i concluded that the groups are the days because there the weather is equal and so they should only be differented by the amount of selling.

### Factoesxtra package with clustering

After i compleded the pca i started with the kmeans clustering again. Here i decided to do a clustering with all numeric variables and one were i build on the results out of the pca before. First i did the kmeans clustering with all numeric attributes. There i begun with the elbow method. It's used to find the optimal amount of clusters. There are serveral methods. I used the "gap_stat" method and concluded out of the plot that i should need 5 clusters. So i computed the kmeans clusters and visualized them with the "fviz_cluster" function. In the plot you saw 5 clusters. One of these clusters was an outlier. I looked up the cluster in the data frame and found out that one attribute has in some rows the number "-999". And i saw that this attribute is one of the attributes i decided to delete because of the pca. So again you see serveral groups of points. And again i concluded that each group must be one day and each point is one customer who bought something on this day. So the clusters are days were the weather is not so different. I mutated the clusters as a factor in a new column of the data frame and tried to find some plot or table with interresting informations. I could't find something and because of the time i had to continue. In the ende i decided to have a look on the average of each numeric attribute for the clusters. And you could see that the clusters are diiferent because the temperature is different. So we have 5 different average temperatures.

In the second kmeans clustering i used the results out of the pca and decided to only use the attributes Gesamtbrutto, TXK, TNK, VPM, TMK, UPM and TGK. With the "fviz_nbclust" function with the "gap_stat" method i tried to find the optimal number of clusters for this case. The method said that i have to use only one cluster and i decided to use another method. This time i used the "wss" method and decided to use 4 clusters. And again i saw groups of points. But this time i had the feeling that this could be some better clustering. So i did the average messurement for the clusters and yes we got a result. We have 4 clusters:

Cluster | avg spend money per customer | vapor pressur | avg days mean tmp | relative humidity |  days max air tmp in 2m | days min air tmp in 2m | days min tmp 5cm
--------|------------------------------|---------------|-------------------|-------------------|-------------------------|------------------------|-----------------
  1     |     3.17                     |     17.75     |       20.14       |        76.3       |           25.24         |          14.4          |    12.4
  2     |     2.72                     |     12.34     |       14.71       |        74.6       |           19.64         |           9.6          |     7.5
  3     |     3.60                     |     14.22     |       13.99       |        88.6       |           16.88         |          11.1          |     9.2
  4     |     3.54                     |      9.41     |        7.10       |        92.5       |            9.34         |           4.0          |     1.9
  
We can see that if the humidity raises and the temperature falls the customers buy more superfood. Cluster 2 could be an outlier because i can remember that in the transition period my offers of different varieties wasn't that high. There could be a dependency between cold and wet weather and the total spend in superfood per customer. But it could also be a normal growth of my company and that the customers acept the producs more and use it more. To find out that we need more data from the whole year.

## Statistical application

After i finished the analysis the idea of an analysis aplication materilized in my brain. So i decided to build one. It should analyse the three different data frames automaticly how i analysed the first one by hand. It should have some methods to choose like the pca and the clustering and some options like the amount of clusters. So i started to program the first analysis into a interactive shiny app. I added some reactive chooseable options and three tabs for the three data frames. Then i tried to combine serveral reactive stuff but it doesn't worked out very well so build it up step by step for each data frame. It now can show us the pca and the clustering for all three data frames but it can't use the results of the pca analysis in the clustering. To fix that i have to spend more time into this. Also the app could have more options but again because of lack of time i choose only this simple features. I come here to an end and don't describe the results for the two other data frames because "NO TIME".

## Reflection

It costs so much time that i could not analyse the other two data frames. So i only answered one of my questions. But i came to a result witch could be a dependency or not. I spend now around 20-25 hours to do the data wrangling, analyse one data frame and build some simple analyse application with simple features for all three data frames. If i spend more than that i easylie could build one good application for my company to analyse the data. Overall it was really fun to do some statistical analyse and the devil is in the detail. 



# Welcome to the project for the math class

I will add my customer data to the weather data and i will try to do a pca (pricipal component analyse), some clustering(kmeans, k-nearest neightbour) and a linear regression on the data. I want to find some relationsships between selling of a product and the weather or some other relationship i dont know. So first of all i will combine the data into one big data frame. To do that we have to load all the different data and do some data wrangling. 

## Questions

**Are there dependencies betweeen the weather and  the sales?**\
**Are there dependencies between the weather and different product sales?**\
**Are there dependencies between the weather and customers?**\

# Data Wrangling

## Load data

```{r}
selling_q3 <- read_xls("Data/Auszahlungen_2019-07-01_2019-09-30.xls")
selling_oct <- read_xls("Data/Auszahlungen_2019-10-01_2019-10-31.xls")
selling_nov <- read_xls("Data/Auszahlungen_2019-11-01_2019-11-30.xls")
product_q3 <- read_xls("Data/Verkaufe_pro_Angebot_2019-07-01_2019-09-30.xls")
product_oct <- read_xls("Data/Verkaufe_pro_Angebot_2019-10-01_2019-10-31.xls")
product_nov <- read_xls("Data/Verkaufe_pro_Angebot_2019-11-01_2019-11-30.xls")
customers_q3 <- read_xls("Data/Verkaufe_Rechnungen_2019-07-01_2019-09-30.xls")
customers_oct <- read_xls("Data/Verkaufe_Rechnungen_2019-10-01_2019-10-31.xls")
customers_nov <- read_xls("Data/Verkaufe_Rechnungen_2019-11-01_2019-11-30.xls")
weather <- read_delim("Data/produkt_klima_tag_20180704_20200104_02564.txt", ";", escape_double = FALSE, trim_ws = TRUE)
```

I loaded some data of my customers, my selling in generell and my selling of each product. So the selling data frame is for the generall selling informations. The produkt data frame is for the informations about each product and how often it got sold. The last data frame is called customers and it holds informations about the customers. Where they are from and how much they bought. And i forgot the weather data frame. It holds the weather information for each day.

## Combine the other months with the first three

To get the whole data we have to combine all the different months.

```{r}
customers <- list(customers_q3, customers_oct, customers_nov) %>%
  bind_rows()
product <- list(product_q3, product_oct, product_nov) %>%
  bind_rows()
selling <- list(selling_q3, selling_oct, selling_nov) %>%
  bind_rows()
```

## Delete the rest

Let's delete the data frames which aren't needed anymore. So the environment is tidied up.

```{r}
rm(customers_q3, customers_oct, customers_nov, product_q3, product_oct, product_nov, selling_q3, selling_oct, selling_nov)
```


## Delete columns which aren't needed

We have to tidy up some columns which are totally useless or the information is doubled.

```{r}
customers <- customers %>%
  select(-`Summe MwSt.`, -`Nummer der Schwärmerei`, -`Summe netto`)
product <- product %>%
  select(-`Summe MwSt.`, -`Summe netto`, -Produktkategorie, -`Produkt Unterkategorie`)
selling <- selling %>%
  select(-`Verkäufe - Mehrwertsteuer`, -`Verkäufe - Summe netto`, -`Servicegebühr - Mehrwertsteuersatz 19%`, -`Servicegebühr - wenn der Rechnungsempfänger die MwSt. nicht in Rechnung stellt`, -`Einkäufe - Summe netto`, -`Einkäufe - Mehrwertsteuer`)
weather <- weather %>%
  select(-STATIONS_ID, -QN_3, -QN_4, -SHK_TAG, -eor, -SDK)
```


## Replace different product names

Because i changed the product names in november i have to replace some product names so the same products have the same names.

```{r}
product$Produktname <- product$Produktname %>%
  str_replace_all("1 Radieschen rot Microgreens unverpackt 40g", "Radieschen rot Microgreens unverpackt (1 × 40 g)")
product$Produktname <- product$Produktname %>%
  str_replace_all("1 Kohlrabi rot Microgreens", "Kohlrabi rot Microgreens unverpackt (1 x 40 g)")
product$Produktname <- product$Produktname %>%
  str_replace_all("Brokkoli Microgreens unverpackt 40g - 40 g", "Brokkoli Microgreens unverpackt (1 x 40 g)")
product$Produktname <- product$Produktname %>%
  str_replace_all("Rote Bete Microgreens - 40 g", "Rote Bete Microgreens unverpackt (1 × 40 g)")
product$Produktname <- product$Produktname %>%
  str_replace_all("1 Mizuna Microgreens unverpackt", "Mizuna Microgreens unverpackt (1 x 40 g)")
product$Produktname <- product$Produktname %>%
  str_replace_all("1 Bockshornklee Microgreens unverpackt", "Bockshornklee Microgreens unverpackt (1 x 40 g)")
product$Produktname <- product$Produktname %>%
  str_replace_all("1 Daikon Rettich unverpackt 40g", "Daikon Rettich unverpackt (1 × 40 g)")
product$Produktname <- product$Produktname %>%
  str_replace_all("1 Radieschen rot Microgreens unverpackt 100g", "Radieschen rot Microgreens unverpackt (1 × 100 g)")
product$Produktname <- product$Produktname %>%
  str_replace_all("1 Sonnenblumen Microgreens unverpackt 100g", "Sonnenblumen Microgreens unverpackt (1 × 100 g)")
product$Produktname <- product$Produktname %>%
  str_replace_all("1 Sonnenblumen Microgreens unverpackt 40g", "Sonnenblumen Microgreens unverpackt (1 × 40 g)")
product$Produktname <- product$Produktname %>%
  str_replace_all("20g Koriander Microgreens unverpackt 20g", "Koriander Microgreens unverpackt (1 × 20 g)")
product$Produktname <- product$Produktname %>%
  str_replace_all("Erbsen Microgreens unverpackt 40g - 40 g", "Erbsen Microgreens unverpackt (1 × 40 g)")
product$Produktname <- product$Produktname %>%
  str_replace_all("Senf Microgreens unverpackt 40g (1 × 40 g)", "Senf Microgreens unverpackt (1 × 40 g)")
```


## Test of product names

```{r}
product %>%
  distinct(Produktname) %>%
  arrange(Produktname)
```

We see that there is still one name doubled. But i could't find the error so we have to use it like this.

## Lets replace different offernumbers

Here we have the same problem like for the productnames. So we have to replace some numbers that we get a equal number for the same product. If we don't do that we have lot's of different numbers for the same product and the analyse does't work very well.

```{r}
product$Angebotsnummer <- product$Angebotsnummer %>%
  str_replace_all("BMICR.fdaf.1", "BMU40.g3va.1")
product$Angebotsnummer <- product$Angebotsnummer %>%
  str_replace_all("DRUNV.fino.1", "DRU40.g3vj.1")
product$Angebotsnummer <- product$Angebotsnummer %>%
  str_replace_all("RROT.f9et.1", "RRMU4.g3vu.1")
product$Angebotsnummer <- product$Angebotsnummer %>%
  str_replace_all("RRU10.fpiv.1", "RRMU4.g3vu.2")
product$Angebotsnummer <- product$Angebotsnummer %>%
  str_replace_all("RRMU1.g3vr.2", "RRMU4.g3vu.2")
product$Angebotsnummer <- product$Angebotsnummer %>%
  str_replace_all("SENF.f9ew.1", "SMU40.g3vw.1")
product$Angebotsnummer <- product$Angebotsnummer %>%
  str_replace_all("SMUNV.finu.1", "SMU40.g3w1.1")
product$Angebotsnummer <- product$Angebotsnummer %>%
  str_replace_all("SMU10.g3vz.1", "SMU40.g3w1.2")
product$Angebotsnummer <- product$Angebotsnummer %>%
  str_replace_all("SMU10.foby.1", "SMU40.g3w1.2")


```

## Test the offernumbers

```{r}
product %>%
  distinct(Angebotsnummer, Produktname) %>%
  arrange(Produktname)
```

As you can see we have now only one offernumber for one product.

## Use lubridate to manipulate the date column

We have to have equal date columns to join the selling data with the weather data. So i use lubridate to change the date column to make easier to join the data.

```{r}
# customers$`Datum der Verteilung` <- ymd_hms(customers$`Datum der Verteilung`) maybe we have to extract only the date
weather$MESS_DATUM <- ymd(weather$MESS_DATUM)

customers <- customers %>%
  mutate(date = date(customers$`Datum der Verteilung`))
product <- product %>%
  mutate(date = date(product$`Datum der Verteilung`))
selling <- selling %>%
  mutate(date = date(selling$`Datum der Verteilung`))
weather <- weather %>%
  mutate(date = date(weather$MESS_DATUM))
```


## Let's add some weather data to the selling informations.

Let's add the weather information for the selling days. Maybe there is a correlation between bad weather and bad selling days. But we will join it in a new data frame so we can analyse the date with weather informations and without.

```{r}
customers_weather <- left_join(customers, weather, by = "date") 
# by = c("column1", "column2") so i don't have to create a new column with is equal in every data frame
product_weather <- left_join(product, weather, by = "date")
selling_weather <- left_join(selling, weather, by = "date")
```


## Let's have a look on the products

```{r}
product %>%
  group_by(Produktname) %>%
  summarise(Verkauft = sum(Menge), Erloes = sum(Gesamtbrutto)) %>%
  ggplot(aes(x = reorder(Produktname, Erloes), y = Verkauft, fill = Erloes)) +
  geom_col() +
  coord_flip()
```

# Statistical analysis

I will try some methods we discussed in class on my data. After that I will look on the result and decide what to do next. Maybe we will get some nice dependencies or maybe not. First i will do some clustering with kmeans and k-Nearest Neighbour. After that i will try a principle component analysis and at least a linear regression. Hopefully i will learn something about my customers and we will get some interesting insights.



## kmeans clustering

Let's try to find some interesting clusters to discribe customer groups or something else. First i will try to get some clusters with the combined customer_weather data. We have to use numeric columns for clusters. So we have to explicid say with columns are used for the clustering. So let's try every column which is numeric. To view the clusters we can use ggplot with a 2d plot.

```{r}
km1 <- kmeans(x = customers_weather[,c("Gesamtbrutto", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")], centers = 2)

df1 <- customers_weather

df1 <- bind_cols(df1, km = as.factor(km1$cluster))

df1 %>% 
  ggplot() +
  geom_point(aes(Gesamtbrutto, TNK, color = km)) +
  geom_point(data = as.data.frame(km1$centers), aes(Gesamtbrutto, TNK), size = 3)
```

We can see that this model doens't work very well. Lets try the others.

```{r}
km2 <- kmeans(x = product_weather[,c("Einzelpreis netto", "Menge", "Gesamtbrutto", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")], centers = 2)

df2 <- product_weather

df2 <- bind_cols(df2, km = as.factor(km2$cluster))

df2 %>% 
  ggplot() +
  geom_point(aes(Gesamtbrutto, TNK, color = km)) +
  geom_point(data = as.data.frame(km2$centers), aes(Gesamtbrutto, TNK), size = 3)
```

Same visualization as the plot before.

```{r}
km3 <- kmeans(x = selling_weather[,c("Verkäufe - Gesamtbetrag", "Einkäufe - Gesamtbetrag", "Gesamtsumme", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")], centers = 3)

df3 <- selling_weather

df3 <- bind_cols(df3, km = as.factor(km3$cluster))

df3 %>% 
  ggplot() +
  geom_point(aes(Gesamtsumme, TNK, color = km)) +
  geom_point(data = as.data.frame(km3$centers), aes(Gesamtsumme, TNK), size = 3)
```

This doesn't work really well. Lets try the factoextra package. Maybe we can find something in there which helps and make it more convinient.

## Factoextra package

### Load package

```{r include=FALSE}
library(FactoMineR)
library(factoextra)
library(corrplot)
```


### PCA

Let's do a principle component analyses to extract only the needed variables to explain 85-90 percent of the varriance.

```{r}
df1_pca <- PCA(df1[,c("Gesamtbrutto", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")],  graph = FALSE)

```
 
 Let's have a look on the eigenvalues and eigenvariances.
 
```{r}
get_eig(df1_pca)
```
  
  We can see that we can fit with 86 % if we only choose 5 dimensions. Let's visualize it to have a better look at it.
  
```{r}
fviz_screeplot(df1_pca, addlabels = TRUE, ylim = c(0, 50))
```
  
  Let's look into the variables. Which one explained most of the variance. Let's extract the results for the variables.
  
```{r}
var <- get_pca_var(df1_pca)
var
```
  
We can print the head of this atributes. First the coordinates, second the contribution.

```{r}
head(var$coord)
```

The coordinates explain the direction and the contribution explained the length of the arrows

```{r}
head(var$contrib)
```

Let's visualize it.

```{r}
fviz_pca_var(df1_pca, col.var = "black")
```

Or with this one.

```{r}
corrplot(var$cos2, is.corr=FALSE)
```

We can see that the tempretures\
(TXK = Days maximum of airtemperatur in 2m hight in °c,\
 TNK = Days minimum of airtemperatur in 2m hight in °c,\
 TMK = Days mean temperature,\
 TGK = Days minimum of airtemperature in 5 cm highth)\
 and the mean of days relative humidity = UPM and the mean of days vapor pressure = VPM explained most of the first dimension.
 So we have to delete some of the columns to handle the data better. At least let's try the bipolar plot.
 
```{r}
fviz_pca_biplot(df1_pca)
```
 
 Now we can see all the different customers and which variable fit to the customers. Each group of points are one selling day with its explicit weather and each point are one customer of this selling day which bought a different amount. We should now minimize the dimensions based on our pca to get a easier handling for the data. 
 
### Clustering
 
 Let's try to cluster again. But this time we will scale our data and we will use factoextra package. Here without using the results of the pca.
 
```{r}
# 1. Loading and preparing data
df1_scaled <- scale(df1[,c("Gesamtbrutto", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")])
# 2. Find the optimal number of clusters
fviz_nbclust(df1_scaled, kmeans, method = "gap_stat")
# 3. Compute k-means
km1_scaled <- kmeans(scale(df1[,c("Gesamtbrutto", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")]), 5, nstart = 25)
# 4. Visualize
fviz_cluster(km1_scaled, data = df1_scaled,
             palette = c("#00AFBB","#2E9FDF", "#E7B800", "#FC4E07", "#000000"),
             ggtheme = theme_minimal(),
             main = "kMeans Clustering"
             )
```
 
 It looks like we have clustered the days with the same weather. Let's add these clusters to the original data.
 
```{r}
df1 <- bind_cols(df1, km_right = as.factor(km1_scaled$cluster))
```
 
 Let's plot some information with the addional cluster column.
 
```{r}
df1 %>%
  distinct(Name, Vorname, Postleitzahl, km_right) %>%
  count(Postleitzahl)

df1 %>%
  distinct(Name, Vorname, Postleitzahl, km_right) %>%
  count(km_right)

df1 %>%
  group_by(date, km_right) %>%
  summarise(verdient = sum(Gesamtbrutto)) %>%
  ggplot(aes(x = date, y = verdient, colour = km_right )) +
  geom_point()
  
```
 
 I couldn't find some good plot with this additional cluster information.
 
```{r}

# doesn't work with the anonymous data set
# df1 %>%
#   group_by(Name, Vorname) %>%
#   summarise(ausgegeben = sum(Gesamtbrutto)) %>%
#   arrange(desc(ausgegeben))
```
 
 Now i try to look at the average of the attributes of a cluster. Maybe we can define the clusters.
 
```{r}
df1 %>%
  group_by(km_right) %>%
  summarize(avg_Gesamtbrutto = mean(Gesamtbrutto), avg_FX = mean(FX), avg_FM = mean(FM), avg_RSK = mean(RSK), avg_RSKF = mean(RSKF), avg_NM = mean(NM), avg_VPM = mean(VPM), avg_PM = mean(PM), avg_TMK = mean(TMK), avg_UPM = mean(UPM), avg_TXK = mean(TXK), avg_TNK = mean(TNK), avg_TGK = mean(TGK))
```
 
 
 I come to an end and my knowledge out of this analyses is that there no dependencies between the weather and the customers because lots of customers had serveral clusters. I think that the amount of data could be more. I should try this analyses on my selling data. Maybe I can find there some dependencies between overall selling and weather. And there should be a method to weight some variables more than others. We can see in the table above that we get 5 clusters. They differences in temperature and humidity. Let's try to use our result out of the pca to minimalize the attributes. So it is more convinient and easier to interpretate.
 
```{r}
# 1. Loading and preparing data
df1_scaled <- scale(df1[,c("Gesamtbrutto", "TXK", "TNK", "VPM", "TMK", "UPM", "TGK")])
# 2. Find the optimal number of clusters
fviz_nbclust(df1_scaled, kmeans, method = "gap_stat")
fviz_nbclust(df1_scaled, kmeans, method = "wss")
# 3. Compute k-means
km1_scaled <- kmeans(scale(df1[,c("Gesamtbrutto", "TXK", "TNK", "VPM", "TMK", "UPM", "TGK")]), 4, nstart = 25)
# 4. Visualize
fviz_cluster(km1_scaled, data = df1_scaled,
             palette = c("#00AFBB","#2E9FDF", "#E7B800", "#FC4E07"),
             ggtheme = theme_minimal(),
             main = "kMeans Clustering"
             )

```
 
 Add this clusters to the table.
 
```{r}
df1 <- bind_cols(df1, km_right2 = as.factor(km1_scaled$cluster))
```
 
  Let's have a look on the average values.
 
```{r}
table1 <- df1 %>%
  group_by(km_right2) %>%
  summarize(avg_Gesamtbrutto = mean(Gesamtbrutto), avg_VPM = mean(VPM), avg_TMK = mean(TMK), avg_UPM = mean(UPM), avg_TXK = mean(TXK), avg_TNK = mean(TNK), avg_TGK = mean(TGK))

DT::datatable(table1)
```
 
 Cluster | avg spend money per customer | vapor pressur | avg days mean tmp | relative humidity |  days max air tmp in 2m | days min air tmp in 2m | days min tmp 5cm
--------|------------------------------|---------------|-------------------|-------------------|-------------------------|------------------------|-----------------
  1     |     3.17                     |     17.75     |       20.14       |        76.3       |           25.24         |          14.4          |    12.4
  2     |     2.72                     |     12.34     |       14.71       |        74.6       |           19.64         |           9.6          |     7.5
  3     |     3.60                     |     14.22     |       13.99       |        88.6       |           16.88         |          11.1          |     9.2
  4     |     3.54                     |      9.41     |        7.10       |        92.5       |            9.34         |           4.0          |     1.9
  
We can see that if the humidity raises and the temperature falls the customers buy more superfood. Cluster 2 could be an outlier because i can remember that in the transition period my offers of different varieties wasn't that high. So it could influenced the total spend per customer. There could be a dependency between cold and wet weather and the total spend in superfood per customer.
 
 
# Statistical analysis app

Now i will try to program an analysis app for all datasets. So this app needs some chooseable datasets and than there have to be some options for the analysis. Let's start and see how far we get.

## Load packages

```{r include=FALSE}
library(plotly)
library(shiny)
library(DT)
library(gridExtra)
```

## Program an application

```{r}
analysis <- c("pca", "k-means")

shinyApp(
  
ui <- fluidPage(
  titlePanel("Interactive analysis application"),
  sidebarLayout(
    sidebarPanel(
      # conditionalPanel(
      #   'input.dataset === "customers_weather"',
        selectInput("select1", "Select your analysis", analysis),
        #sliderInput("slider1", label = h3("Choose your number of clusters"), min = 1, max = 15, value = 2)
        uiOutput("ui1")
      # ),
      # conditionalPanel(
      #   'input.dataset === "selling_weather"',
      #   selectInput("select1", "Select your analysis", analysis),
      #   #sliderInput("slider1", label = h3("Choose your number of clusters"), min = 1, max = 15, value = 2)
      #   uiOutput("ui2")
      # ),
      # conditionalPanel(
      #   'input.dataset === "product_weather"',
      #   selectInput("select1", "Select your analysis", analysis),
      #   #sliderInput("slider1", label = h3("Choose your number of clusters"), min = 1, max = 15, value = 2)
      #   uiOutput("ui3")
      # )
    ),
    
      # Mainpanel with 3 different tabs and plots
    
    mainPanel(
      tabsetPanel(
        id = "dataset",
        tabPanel(
          "customers_weather",
          id = "customers_weather",
          dataTableOutput("table1"),
          plotlyOutput("customers_plot1"),
          plotlyOutput("customers_plot2"),
          plotOutput("customers_plot3")
        ),
        tabPanel(
          "selling_weather",
          id = "selling_weather",
          dataTableOutput("table2"),
          plotlyOutput("selling_plot1"),
          plotlyOutput("selling_plot2"),
          plotOutput("selling_plot3")
        ),
        tabPanel(
          "product_weather",
          id = "product_weather",
          dataTableOutput("table3"),
          plotlyOutput("product_plot1"),
          plotlyOutput("product_plot2"),
          plotOutput("product_plot3")
        )
      )
    )
    
  )
  
),

server <- function(input, output, session) {
  
 
    # reactive ui function which expant more options if you change the analysis
  
  output$ui1 <- renderUI({
    if (is.null(input$select1))
      return()
    switch (input$select1,
      "pca" = NULL,
      "k-means" = sliderInput("slider1", label = h3("Choose your number of clusters"), min = 1, max = 15, value = 2)
    )
  })
  
    # the reactive table switcher which switches the data sets if you switch the tabs
    # it worked but only without the reactive ui function
  
  table_switcher <- reactive({
    switch (input$dataset,
      "customers_weather" = datatable(customers_weather),
      "selling_weather" = datatable(selling_weather),
      "product_weather" = datatable(product_weather)
    )
  })
  
    # some tries to find the error
  
  # output$ui2 <- renderUI({
  #   if (is.null(input$select1))
  #     return()
  #   switch (input$select1,
  #     "pca" = NULL,
  #     "k-means" = sliderInput("slider1", label = h3("Choose your number of clusters"), min = 1, max = 15, value = 2)
  #   )
  # })
  # output$ui3 <- renderUI({
  #   if (is.null(input$select1))
  #     return()
  #   switch (input$select1,
  #     "pca" = NULL,
  #     "k-means" = sliderInput("slider1", label = h3("Choose your number of clusters"), min = 1, max = 15, value = 2)
  #   )
  # })  
  
    # reactive functions for the customers data analysis
    # it shows different plots dependend on what you have choosen
    # you could do it more compact with a switch function wich is based on the tabs you choose
    # so you would only need 3 reactive switch functions for the three plots and 3 switch functions for the 3 different data sets instead of 9 functions
  
  switcher1 <- reactive({
    switch (input$select1,
      "pca" = {
        pca_cw <- PCA(customers_weather[,c("Gesamtbrutto", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")],  graph = FALSE)
        fviz_screeplot(pca_cw, addlabels = TRUE, ylim = c(0, 50))
      },
      "k-means" = {
        customers_weather_scaled <- scale(customers_weather[,c("Gesamtbrutto", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")])
        fviz_nbclust(customers_weather_scaled, kmeans, method = "gap_stat")
      }
    )
  })
  
  switcher2 <- reactive({
    switch (input$select1,
      "pca" = {
        pca_cw <- PCA(customers_weather[,c("Gesamtbrutto", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")],  graph = FALSE)
        fviz_pca_var(pca_cw, col.var = "black")
      },
      "k-means" = {
        customers_weather_scaled <- scale(customers_weather[,c("Gesamtbrutto", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")])
        km1_scaled <- kmeans(scale(customers_weather[,c("Gesamtbrutto", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")]), input$slider1, nstart = 25)
        fviz_cluster(km1_scaled, data = customers_weather_scaled,
                     ggtheme = theme_minimal(),
                     main = "kMeans Clustering"
                     )
      }
    )
  })
  
  switcher3 <- reactive({
    switch (input$select1,
      "pca" = {
        pca_cw <- PCA(customers_weather[,c("Gesamtbrutto", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")],  graph = FALSE)
        var_cw <- get_pca_var(pca_cw)
        corrplot(var_cw$cos2, is.corr=FALSE)
      },
      "k-means" = NULL
    )
  })
  
    # reactive functions for the selling data analysis
    # it shows different plots dependend on what you have choosen
  
  switcher4 <- reactive({
    switch (input$select1,
      "pca" = {
        pca_sw <- PCA(selling_weather[,c("Verkäufe - Gesamtbetrag", "Einkäufe - Gesamtbetrag", "Gesamtsumme", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")],  graph = FALSE)
        fviz_screeplot(pca_sw, addlabels = TRUE, ylim = c(0, 50))
      },
      "k-means" = {
        selling_weather_scaled <- scale(selling_weather[,c("Verkäufe - Gesamtbetrag", "Einkäufe - Gesamtbetrag", "Gesamtsumme", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")])
        fviz_nbclust(selling_weather_scaled, kmeans, method = "gap_stat")
      }
    )
  })
  
  switcher5 <- reactive({
    switch (input$select1,
      "pca" = {
        pca_sw <- PCA(selling_weather[,c("Verkäufe - Gesamtbetrag", "Einkäufe - Gesamtbetrag", "Gesamtsumme", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")],  graph = FALSE)
        fviz_pca_var(pca_sw, col.var = "black")
      },
      "k-means" = {
        selling_weather_scaled <- scale(selling_weather[,c("Verkäufe - Gesamtbetrag", "Einkäufe - Gesamtbetrag", "Gesamtsumme", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")])
        km2_scaled <- kmeans(scale(selling_weather[,c("Verkäufe - Gesamtbetrag", "Einkäufe - Gesamtbetrag", "Gesamtsumme", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")]), input$slider1, nstart = 25)
        fviz_cluster(km2_scaled, data = selling_weather_scaled,
                     ggtheme = theme_minimal(),
                     main = "kMeans Clustering"
                     )
      }
    )
  })
  
  switcher6 <- reactive({
    switch (input$select1,
      "pca" = {
        pca_sw <- PCA(selling_weather[,c("Verkäufe - Gesamtbetrag", "Einkäufe - Gesamtbetrag", "Gesamtsumme", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")],  graph = FALSE)
        var_sw <- get_pca_var(pca_sw)
        corrplot(var_sw$cos2, is.corr=FALSE)
      },
      "k-means" = NULL
    )
  })
  
    # reactive functions for the product data analysis
    # it shows different plots dependend on what you have choosen
  
  switcher7 <- reactive({
    switch (input$select1,
      "pca" = {
        pca_pw <- PCA(product_weather[,c("Einzelpreis netto", "Menge", "Gesamtbrutto", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")],  graph = FALSE)
        fviz_screeplot(pca_pw, addlabels = TRUE, ylim = c(0, 50))
      },
      "k-means" = {
        product_weather_scaled <- scale(product_weather[,c("Gesamtbrutto", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")])
        fviz_nbclust(product_weather_scaled, kmeans, method = "gap_stat")
      }
    )
  })
  
  switcher8 <- reactive({
    switch (input$select1,
      "pca" = {
        pca_pw <- PCA(product_weather[,c("Einzelpreis netto", "Menge", "Gesamtbrutto", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")],  graph = FALSE)
        fviz_pca_var(pca_pw, col.var = "black")
      },
      "k-means" = {
        product_weather_scaled <- scale(product_weather[,c("Einzelpreis netto", "Menge", "Gesamtbrutto", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")])
        km3_scaled <- kmeans(scale(product_weather[,c("Einzelpreis netto", "Menge", "Gesamtbrutto", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")]), input$slider1, nstart = 25)
        fviz_cluster(km3_scaled, data = product_weather_scaled,
                     ggtheme = theme_minimal(),
                     main = "kMeans Clustering"
                     )
      }
    )
  })
  
  switcher9 <- reactive({
    switch (input$select1,
      "pca" = {
        pca_pw <- PCA(product_weather[,c("Einzelpreis netto", "Menge", "Gesamtbrutto", "TXK", "TNK", "FX", "FM", "RSK", "RSKF", "NM", "VPM", "PM", "TMK", "UPM", "TGK")],  graph = FALSE)
        var_pw <- get_pca_var(pca_pw)
        corrplot(var_pw$cos2, is.corr=FALSE)
      },
      "k-means" = NULL
    )
  })
  
    # output of the data tables in with a reactive function
  
  output$table <- renderDataTable({
    table_switcher()
  })
  
    # Output of the data tables, you could use it as an input to do other stuff
    # i tried to use only one function but it doesn't worked very well
  
  output$table1 <- renderDataTable({
    datatable(customers_weather)
  })
  output$table2 <- renderDataTable({
    datatable(selling_weather)
  })
  output$table3 <- renderDataTable({
    datatable(product_weather)
  })
  
    # output of the customers analysis
    # in there is a function which swiches between pca and cluster analysis
  
  output$customers_plot1 <- renderPlotly({
    switcher1()
  })
  output$customers_plot2 <- renderPlotly({
    switcher2()
  })
    output$customers_plot3 <- renderPlot({
    switcher3()
  })
    
    # i tried to do it reactive with a function so i dont have to use 9 outputs but it didn't work
    # output of the selling analysis
    # in there is a function which swiches between pca and cluster analysis
    
  output$selling_plot1 <- renderPlotly({
    switcher4()
  })
  output$selling_plot2 <- renderPlotly({
    switcher5()
  })
    output$selling_plot3 <- renderPlot({
    switcher6()
  })
    
    # output of the product analysis
    # in there is a function which swiches between pca and cluster analysis
    
  output$product_plot1 <- renderPlotly({
    switcher7()
  })
  output$product_plot2 <- renderPlotly({
    switcher8()
  })
    output$product_plot3 <- renderPlot({
    switcher9()
  })
  
},
options = list(height = 1500)
)
```

## Reflection

The application is nice and it shows you easiely the results of this two methods. Now i could implement some more methods and options. For example some different elbow methods, different clustering, the number of iterations or a complete new method like the linear regression. However, it tooks some hours to type this code. I would be really happy if i could get the code smaller and more compressed but that doesn't worked out well. You can overview the results and see that there some variables witch i could delete because there variance is really low. To see some informations out of the clustering we have to do some more plots with different questionaries. The other two data sets had different cluster and maybe there are some good insights in the clustering. To get relevant information out of unsorted and not explored data you always have to offer some time to find the right questions and to get the results. I think if i offer one or two more weeks into this project i could find some interresting informations.
